{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training end evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os, sys\n",
    "# Add the 'scripts' directory to the Python path for module imports\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'scripts')))\n",
    "# Set max rows and columns to display\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/prepared_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Devide the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use train_test_split from scikit-learn to split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=df['TotalPremium'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preparation import EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = EDA(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df.drop(columns=['TotalPremium'])\n",
    "y = df['TotalPremium']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test=eda.split_data(\"TotalPremium\")\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the model generalizes well to unseen data, the dataset was split into training and test sets. Typically, the split is done in either a 70:30 or 80:20 ratio.\n",
    "\n",
    "- Train Set: Used to train the model.\n",
    "- Test Set: Used to evaluate model performance on unseen data.\n",
    "**Modeling Techniques**\n",
    "\n",
    "The following machine learning models were trained using the training data:\n",
    "\n",
    "- Linear Regression: A baseline regression model that assumes a linear - relationship between the features and the target.\n",
    "- Decision Tree: A non-linear model that learns simple decision rules inferred from the data.\n",
    "- Random Forest: An ensemble model that combines multiple decision trees to improve prediction accuracy and generalization.\n",
    "- XGBoost: A powerful boosting algorithm that iteratively improves model performance by focusing on difficult-to-predict samples.\n",
    "\n",
    "#### Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse_lr:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lr:.2f}\")\n",
    "print(f\"R-squared (R2): {r2_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_lr, alpha=0.6, color=\"blue\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_lr\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred_lr, residuals, alpha=0.6, color=\"purple\")\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featured Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(lr_model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=X_train.values, \n",
    "    feature_names=X_train.columns, \n",
    "    mode=\"regression\"\n",
    ")\n",
    "\n",
    "# Explain a single prediction\n",
    "exp = explainer.explain_instance(X_test.iloc[0].values, lr_model.predict, num_features=5)\n",
    "exp.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decsision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "import numpy as np\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "rmse_dt = np.sqrt(mse_dt)\n",
    "mape_dt = np.mean(np.abs((y_test - y_pred_dt) / y_test)) * 100\n",
    "\n",
    "\n",
    "\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "# Use cross-validation to ensure the model generalizes well\n",
    "cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print out validation score\n",
    "print(\"Cross-Validated MSE:\", -np.mean(cv_scores))\n",
    "print(\"RMSE\",rmse_dt)\n",
    "print(\"MAPE\",mape_dt)\n",
    "print(\"R²\", r2_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision Tree Structure\n",
    "Visualize the tree structure to understand how splits are made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Limit the tree depth for plotting\n",
    "max_plot_depth = 3  # Adjust depth for visualization\n",
    "\n",
    "# Generate a text representation for a quick overview (optional)\n",
    "tree_text = export_text(dt_model, feature_names=list(X_train.columns))\n",
    "print(\"Textual representation of the decision tree:\")\n",
    "print(tree_text)\n",
    "\n",
    "# Plot the decision tree with limited depth\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_model, \n",
    "    max_depth=max_plot_depth,  # Limit depth of visualization\n",
    "    filled=True, \n",
    "    feature_names=X_train.columns, \n",
    "    rounded=True\n",
    ")\n",
    "plt.title(f\"Decision Tree (max_depth={max_plot_depth})\")\n",
    "plt.savefig(\"decision_tree_limited_depth.png\", dpi=300)  # Save the plot for later viewing\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Importance:\n",
    "\n",
    "Decision Trees provide feature importance scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dt_model.feature_importances_\n",
    "plt.barh(X_train.columns, importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance for Decision Tree\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prediceted vs Actual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred_dt, alpha=0.7)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_dt\n",
    "plt.scatter(y_pred_dt, residuals, alpha=0.7)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Generate SHAPE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset of the test data to speed up the explanation\n",
    "X_test_subset = X_test.sample(n=100, random_state=42)  # Sample 1000 rows from X_test\n",
    "\n",
    "# Initialize KernelExplainer (slower but more flexible)\n",
    "explainer = shap.KernelExplainer(dt_model.predict, X_test_subset)\n",
    "\n",
    "# Compute SHAP values for the subset of the test data\n",
    "shap_values = explainer.shap_values(X_test_subset)\n",
    "\n",
    "# Plot summary\n",
    "shap.summary_plot(shap_values, X_test_subset, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Visualize Individual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# Generate the force plot for a single instance\n",
    "force_plot = shap.force_plot(\n",
    "    explainer.expected_value, \n",
    "    shap_values[0, :], \n",
    "    X_test.iloc[0, :]\n",
    ")\n",
    "\n",
    "# Save the plot to an HTML file\n",
    "shap.save_html(\"../data/force_plot.html\", force_plot)\n",
    "\n",
    "print(\"The SHAP force plot has been saved to 'force_plot.html'. Open this file in your browser to view the visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. LIME Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    X_train.values, \n",
    "    training_labels=y_train.values, \n",
    "    feature_names=X_train.columns.tolist(), \n",
    "    mode='regression'\n",
    ")\n",
    "\n",
    "explanation = explainer.explain_instance(X_test.iloc[0, :].values, dt_model.predict)\n",
    "explanation.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reandome Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse_rf = mse_rf ** 0.5\n",
    "\n",
    "# R-Squared (R²)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MAE: {mae_rf}\")\n",
    "print(f\"MSE: {mse_rf}\")\n",
    "print(f\"RMSE: {rmse_rf}\")\n",
    "print(f\"R²: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Residual plot:\n",
    "visualize residual to detect pattern and outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "residuals = y_test - y_pred_rf\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred_rf, y=residuals)  # Use x= and y=\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Actual vs residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred_rf, y=residuals)  # Use x= and y=\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf_model.feature_importances_})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Interprate with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Select a smaller sample or cluster\n",
    "sample_X_test = shap.kmeans(X_test, 50)  # Representative 50 samples\n",
    "\n",
    "# Convert DenseData to pandas DataFrame\n",
    "sample_X_test_df = sample_X_test.data  # Get the actual data array\n",
    "sample_X_test_df = pd.DataFrame(sample_X_test_df, columns=X_test.columns)  # Convert to DataFrame\n",
    "\n",
    "# Run SHAP analysis\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(sample_X_test_df)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, sample_X_test_df, feature_names=X_test.columns)\n",
    "\n",
    "# Force plot for the first sample\n",
    "#shap.force_plot(explainer.expected_value[1], shap_values[1][0], sample_X_test_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Interprate with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, mode='regression')\n",
    "explanation = explainer.explain_instance(X_test.iloc[0].values, rf_model.predict)\n",
    "\n",
    "# Display the explanation for a single instance\n",
    "explanation.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse_xgb = mse_xgb ** 0.5\n",
    "\n",
    "# R-Squared (R²)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"MAE: {mae_xgb}\")\n",
    "print(f\"MSE: {mse_xgb}\")\n",
    "print(f\"RMSE: {rmse_xgb}\")\n",
    "print(f\"R²: {r2_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corss validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the model\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "# Initialize KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rmse_list = []\n",
    "\n",
    "# Manually perform cross-validation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Fit model\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict and calculate MSE\n",
    "    y_pred = xgb_model.predict(X_val_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Calculate the mean RMSE\n",
    "cv_rmse = np.mean(rmse_list)\n",
    "print(f\"Cross-Validated RMSE: {cv_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Residual plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_xgb\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred_xgb, y=residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Residual vs predicted plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x= y_test, y=y_pred_xgb)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot feature importance\n",
    "xgb.plot_importance(xgb_model, importance_type='weight', max_num_features=10)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Using matplotlib\n",
    "# Get feature importance\n",
    "feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_model.feature_importances_})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Interprate using SHAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Sample 100 representative data points\n",
    "sample_X_test = shap.kmeans(X_test, 100)\n",
    "\n",
    "# Extract the underlying numpy array from the DenseData\n",
    "sample_X_test_array = sample_X_test.data\n",
    "\n",
    "# Convert the sample_X_test array to xgboost.DMatrix format\n",
    "sample_X_test_dmatrix = xgb.DMatrix(sample_X_test_array)\n",
    "\n",
    "# Run SHAP TreeExplainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(sample_X_test_dmatrix)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, sample_X_test_array, feature_names=X_test.columns)\n",
    "\n",
    "# Force plot for a single prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], sample_X_test_array[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Interprate using LIME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LIME explainer\n",
    "explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, mode='regression')\n",
    "\n",
    "# Choose an instance to explain\n",
    "explanation = explainer.explain_instance(X_test.iloc[0].values, xgb_model.predict)\n",
    "\n",
    "# Display the explanation for a single instance\n",
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. #### Show All Models with Corresponding Evaluation Scores as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Store evaluation metrics\n",
    "model_metrics = []\n",
    "\n",
    "# Train and evaluate models\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    model_metrics.append({\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R²\": r2\n",
    "    })\n",
    "\n",
    "# Create DataFrame for easy comparison\n",
    "model_comparison_df = pd.DataFrame(model_metrics)\n",
    "print(model_comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interpretation**\n",
    "1. Linear Regression\n",
    "\n",
    "\n",
    "    - The highest error metrics (MSE, RMSE) indicate that Linear Regression performs poorly on the dataset.\n",
    "    - The low R² value (0.43) means that only 43% of the variance in the target variable is explained by the model.\n",
    "    - Insight: Linear Regression struggles to capture the complexity in the data and is likely not the best fit for this problem, possibly due to nonlinear relationships in the data.\n",
    "2. Decision Tree\n",
    "\n",
    "\n",
    "    - The lowest MAE and RMSE suggest that Decision Tree captures the data patterns well and makes accurate predictions.\n",
    "    - The high R² value (0.86) indicates that 86% of the variance in the target variable is explained by the model.\n",
    "    - Insight: The Decision Tree model handles nonlinear patterns effectively, providing a good balance between complexity and performance.\n",
    "3. Random Forest\n",
    "\n",
    "\n",
    "    - Similar metrics to the Decision Tree model, with slightly lower MSE and RMSE, indicating slightly better generalization.\n",
    "    - Insight: Random Forest, being an ensemble of Decision Trees, reduces overfitting and delivers consistent performance. It slightly outperforms Decision Tree due to its averaging mechanism.\n",
    "4. XGBoost\n",
    "\n",
    "\n",
    "    - The higher error metrics (MSE, RMSE) compared to Decision Tree and Random Forest indicate that XGBoost did not perform as well on this dataset.\n",
    "    - The moderate R² value (0.60) means that 60% of the variance in the target variable is explained by the model.\n",
    "    - Insight: While XGBoost is powerful for many datasets, it may require additional hyperparameter tuning or be less suited for this particular dataset's structure.\n",
    "**Summary and Recommendation**\n",
    "- **_Best Performer:_** Random Forest has the best overall balance of low error metrics and high R², making it the top choice for this dataset.\n",
    "- **_Close Competitor:_** Decision Tree also performs well, with metrics similar to Random Forest, but it might overfit compared to Random Forest.\n",
    "- **_Underperformers:_** Linear Regression and XGBoost struggle, with Linear Regression failing to capture nonlinear relationships and XGBoost needing further optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. #### Visualize All Models to Make it Simple for Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for plotting\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R²']\n",
    "comparison_data = model_comparison_df.set_index('Model')[metrics]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_data.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Model Comparison: Evaluation Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. #### SHAP Interpretation for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer for each model\n",
    "shap_values_dict = {}\n",
    "for model_name, model in models.items():\n",
    "    if model_name in [\"Linear Regression\"]:\n",
    "        explainer = shap.Explainer(model, X_train)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    shap_values_dict[model_name] = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plots for each model\n",
    "for model_name, shap_values in shap_values_dict.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n",
    "    plt.title(f\"SHAP Summary for {model_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. #### LIME Interpretation for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explanations = {}\n",
    "for model_name, model in models.items():\n",
    "    explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, mode='regression')\n",
    "    explanation = explainer.explain_instance(X_test.iloc[0].values, model.predict)\n",
    "    lime_explanations[model_name] = explanation\n",
    "\n",
    "# Show LIME explanations for the first test instance\n",
    "for model_name, explanation in lime_explanations.items():\n",
    "    explanation.show_in_notebook(show_table=True, show_all=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
